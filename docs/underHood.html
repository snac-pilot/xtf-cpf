<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html
PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" lang="EN">

<head>
  <title>XTF: Under the Hood</title>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8"></meta>
  <link rel="stylesheet" href="library/xtfDocStyle.css" type="text/css" />
</head>

<body bgcolor="#FFFFFF" text="#101010" >

<div class="BaseStyle"> 
  <p class="DocTitle" align="center">XTF: Under the Hood</p>
  <p class="Heading1">Table of Contents</p>
  <!-- - - - - - - - - - - - - - - - - - - - - - -->
  <ul class="NoBullets">
    <li> <b><a href="#Introduction">Introduction</a></b> <br/><br/></li>
    <li> <b><a href="#Documents">Document Processing</a></b> 
      <ul class="NoBullets">
        <li><a href="#Documents_Text">Text</a></li>
        <li><a href="#Documents_Meta">Meta-data</a></li>
        <li><a href="#Documents_Tokenizing">Tokenizing</a></li>
        <li><a href="#Documents_Proximity">Proximity and "Slop"</a></li>
        <li><a href="#Documents_Chunking">Chunking</a></li>
        <li><a href="#Documents_StopWords">Stop Words and Bi-grams</a></li>
      </ul>
      <br/>
    </li>
    <li> <b><a href="#QueryOps">Query Operations and What They Do</a></b> 
      <ul class="NoBullets">
        <li><a href="#QueryOps_User">Interpreting User Queries</a></li>
        <li><a href="#QueryOps_Text">Text Query Operations</a></li>
        <li>
          <ul class="NoBullets">
            <li><a href="#QueryOps_Text_TERMWILD">TERM and Wildcard Queries</a></li>
            <li><a href="#QueryOps_Text_AND">AND</a> and <a href="#QueryOps_Text_OR">OR</a> 
              Queries</li>
            <li><a href="#QueryOps_Text_PHRASE">PHRASE</a> and <a href="#QueryOps_Text_NEAR">NEAR</a> 
              Queries</li>
            <li><a href="#QueryOps_Text_NOT">NOT Clause</a></li>
          </ul>
        </li>
        <li><a href="#QueryOps_Meta">Meta-data Query Operations</a></li><li>
          <ul class="NoBullets">
            <li><a href="#QueryOps_Meta_TERMWILD">TERM and Wildcard Queries</a></li>
            <li><a href="#QueryOps_Meta_RANGE">RANGE Query</a></li>
            <li><a href="#QueryOps_Meta_AND">AND</a> and <a href="#QueryOps_Meta_OR">OR</a> 
              Queries</li>
            <li><a href="#QueryOps_Meta_PHRASE">PHRASE</a> and <a href="#QueryOps_Meta_NEAR">NEAR</a> 
              Queries</li>
            <li><a href="#QueryOps_Meta_NOT">NOT Clause</a></li>
          </ul>
        </li>
        <li><a href="#QueryOps_StopWords">Stop Words in Queries</a></li>
      </ul>
      <br/>
    </li>
    <li> <b><a href="#Marking">Term and Hit Marking</a></b> 
      <ul class="NoBullets">
        <li><a href="#Marking_Snippets">Snippet Formation and Marking</a></li>
        <li><a href="#Marking_Context">Hits in their Original Context</a></li>
        <li><a href="#Marking_Span">Spanning XML Tags</a></li>
        <li><a href="#Marking_Stopwords">Special Rules for Marking Stop Words</a></li>
      </ul>
      <br/>
    </li>
    <li> <b><a href="#Scoring">Hit Scoring</a></b> 
      <ul class="NoBullets">
        <li><a href="#Scoring_Text">Text Hit Scoring</a></li>
        <li><a href="#Scoring_Meta">Meta-data Hit Scoring</a></li>
        <li><a href="#Scoring_Documents">Combined Document Score</a></li>
      </ul>
      <br/>
    </li>
    <li> <b><a href="#Spelling">Spelling Correction</a></b> 
      <ul class="NoBullets">
        <li><a href="#Spelling_Strategy">Choosing an Index-based Strategy</a></li>
        <li><a href="#Spelling_CorrectionAlg">Correction Algorithm</a></li><li>
          <ul class="NoBullets">
            <li><a href="#Spelling_Candidates">Which Words to Consider?</a></li>
            <li><a href="#Spelling_Scoring">Ranking the Candidates</a></li>
            <li><a href="#Spelling_MultiWord">Multi-word Correction</a></li>
            <li><a href="#Spelling_Sanity">Sanity Checks</a></li>
          </ul>
        </li>
        <li><a href="#Spelling_DictCreation">Dictionary Creation</a></li><li>
          <ul class="NoBullets">
            <li><a href="#Spelling_Incremental">Incremental Approach</a></li>
            <li><a href="#Spelling_DataStruct">Data Structures</a></li>
          </ul>
        </li>
      </ul>
      <br/>
    </li>
    <li> <b><a href="#LazyFiles">Lazy Trees</a></b> 
      <ul class="NoBullets">
        <li><a href="#Large_Documents">The Problem of Large Documents</a></li>
        <li><a href="#What_is_lazy">What is a &quot;Lazy Tree&quot;?</a></li>
        <li><a href="#Search_results_in_context">Search Results in Context</a></li>
        <li><a href="#Stylesheet_considerations">Stylesheet Considerations</a></li>
      </ul>
      <br/>
    </li>
  </ul>
  <p class="Heading1">Introduction<a name="Introduction"></a></p>
  <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
  This document goes into moderate depth on the actual operation of the e<b>X</b>tensible 
  <b>T</b>ext <b>F</b>ramework (<b><i>XTF</i></b>.) This document is <i>not</i> 
  an overview of the <b><i>XTF</i></b> system, but rather delves into some nitty-gritty 
  details of how the system performs its tasks. The reader is encouraged to first 
  read the <a href="deployment.html"> XTF Deployment 
  Guide</a> and have a solid understanding of how data flows through the <b><i>XTF</i></b> 
  servlets, both <b><i>crossQuery</i></b> and <b><i>dynaXML</i></b>. 
  <p class="Heading1">Document Processing<a name="Documents"></a></p>
  <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
  The bulk of the <b><i>XTF</i></b> system is concerned with searching XML documents 
  for text in various ways, and displaying the results in several forms. A brute-force 
  search of each word in each document, every time a user made a query, would 
  be extremely inefficient, so the <b><i> textIndexer</i></b> tool is used to 
  compile all of the documents into what is called an "<i>inverted index</i>". 
  In essence, this index is similar to that in the back of a book: for each word, 
  it points to all the locations that word appears in all of the documents that 
  have been indexed.<br/>
  <br/>
  The following sections discuss the details of how the <b><i>textIndexer</i></b> 
  dissects and digests documents, and cover a few basic concepts (such as <i>term</i>, 
  <i>proximity</i>, and <i>stop word</i>) necessary to understand the entire system.<br/>
  <br/>
  But first, some useful definitions are needed. The <b><i>XTF</i></b> system 
  views each XML document as containing two major types of data: (1) Full text, 
  and (2) Meta-data.<br/>
  <br/>
  <div class="IndentL"> <b>XTF Definition:</b> <i>Full text</i>, n. All text within 
    an XML document, except actual element definitions and their attributes.<br/>
    <br/>
    <b>XTF Definition:</b> <i>Meta-data</i>, n. Short data fields within an XML 
    document which describe the entire document, for example: title, author, subject, 
    publication date and publisher, access rights, etc.<br/>
    <br/>
  </div>
  This distinction mainly reflects the way the two types of data are used. Typically, 
  meta-data fields are searched "database style". For instance, if one were looking 
  for any book about Mark Twain published after 1912, then one would search the 
  publication date and subject meta-data fields.<br/>
  <br/>
  By contrast, the full text is typically searched "shotgun style", when one is 
  looking for any use of a word or phrase in any book. For example, one might 
  be interested in every reference to Mark Twain. Of course, the two types of 
  queries can be combined in useful ways, for instance if one were interested 
  in any mention of Mark Twain in books published after 1960.<br/>
  <br/>
  <div class="IndentL"> 
    <p class="Heading2">Text Processing<a name="Documents_Text"></a></p>
    <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
    The <b><i>textIndexer</i></b> extracts the text from each XML document, locating 
    each word and storing its position in the inverted index. These words are 
    called "terms".<br/>
    <br/>
    <b>XTF Definition:</b> <i>term</i>, n. A single indexable word. Most punctuation 
    marks are not considered terms, but may occasionally appear within terms. 
    For example, consider the string "O'Reilly". <b><i>XTF</i></b> considers this 
    to be a single term, not two terms as in "O" and "Reilly". See <a href="#Documents_Tokenizing"> 
    Tokenizing</a> below for details of how terms are parsed.<br/>
    <br/>
    In the following sample XML fragment, all the terms are <u><b>underlined</b></u>.<br/>
    <br/>
    <pre class="Sample">&lt;head id="chapter2"&gt;
    &lt;p&gt;"<u><b>This</b></u> <u><b>is</b></u> <u><b>the</b></u> <u><b>day</b></u> <u><b>of</b></u> <u><b>man</b></u>'s <u><b>greatest</b></u> <u><b>peril</b></u>."&lt;/p&gt;
    &lt;footnote&gt;<u><b>Bekins</b></u>, <u><b>1986</b></u>&lt;/footnote&gt;
&lt;/head&gt;
</pre>
    As many documents may be book-length, this body of text could be extremely 
    large, containing tens or hundreds of thousands of terms. <b><i>XTF</i></b> 
    imposes no limits on the length of the text or the number of terms within 
    it.<br/>
    <br/>
    Of course the text exists within the context of XML elements; these elements 
    and their attributes are not considered part of the document text and are 
    not indexed, with one exception. A special attribute (<span class= "code">xtf:sectionType</span>) 
    can be used to associate a type with block of text; the section type is recorded 
    with all the terms in that block, and text queries can later be restricted 
    to certain blocks based on their section types.<br/>
    <br/>
    <p class="Heading2">Meta-data Processing<a name="Documents_Meta"></a></p>
    <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
    As mentioned earlier, meta-data consists of small fields describing an XML 
    document as a whole. Examples might be a book's publication date and publisher, 
    author(s), subject keywords, etc.<br/>
    <br/>
    <b><i>XTF</i></b> provides a very simple model of meta-data: each document 
    may have any number of meta-data fields, each with a name and a textual value. 
    A given field name may be repeated; each associated text value will be considered 
    one unit of meta-data. Note that structured meta-data (i.e. sub-fields within 
    fields) is not supported.<br/>
    <br/>
    Each meta-data field is scanned for terms, and each term and its position 
    are recorded in the inverted index. Typically these fields contain dozens 
    or rarely hundreds of terms. Longer blocks, while supported, are discouraged 
    as they are inefficient to process. 
    <p class="Heading2">Tokenizing<a name="Documents_Tokenizing"></a></p>
    <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
    <p>As mentioned earlier, all text (whether part of a meta-data field or the 
      full text of a document) is broken into <b>terms</b>.<br/>
      <br/>
      <b>XTF Definition: </b><i>tokenize</i>, v., to break a string of text into 
      discrete tokens, or "terms".<br/>
      <br/>
      <b><i>XTF</i></b>, as it is based on the Lucene search toolkit, uses an 
      XTF specific derivative of the Lucene standard tokenizer, which makes a 
      fair effort at identifying terms in the source text regardless of language. 
      For the most part, a single term consists of one or more of the following 
      characters:</p>
    
     <div class="IndentL">
       <ul>
         <li> 
           Western alphabetic characters such as <b>A</b>, 
           <b>B</b>, <b>C</b>, <b>d</b>, <b>e</b>, <b>f</b>, <b>Ö</b>, <b>Ç</b>,...
         </li>
         <li> 
           Arabic numerals such as <b>1</b>, <b>2</b>, <b>3</b>, 
           ...
         </li>
         <li> 
           Non-breaking symbols, such as <b>&amp;</b>, <b>@</b>, 
           and <b>'</b> (apostrophe)
         </li>
         <li>
           The underscore character (<b> _</b> )
         </li>
       </ul>
     </div>

    <p>By contrast, the XTF system considers all traditional Chinese, Japanese, 
      and Korean characters to be <i><b>logograms </b></i> (complete words in 
      and of themselves) and treats each separate character as a term. Some Western 
      symbols are also treated as logograms by the XTF system and are treated 
      as separate terms. These symbols include:</p>
  
    <div class="IndentL">
      <ul>
        <li> 
          Fractions symbols, such as <b>¼</b>, <b>½</b>, <b>¾</b>, ...
        </li>
        <li> 
          Monetary symbols, such as <b>$</b>, <b>£</b>, <b>¥</b>, ...
        </li>
        <li> 
          Mathematical symbols, such as <b>+</b>, <b>&gt;</b>, 
          <b>=</b>, ...
        </li>
        <li> 
          Trademark or copyright symbology, such as <b>©</b>, 
          <b>®</b>, and <b>™</b>
        </li>
      </ul>
    </div>
      
    <p>Three other western symbols, the period (<b> .</b> ), forward slash ( <b>/</b> 
      ), and dash (<b> -</b> ) are treated differently depending on the context 
      in which they are used. For example, if these characters appear in an acronym 
      like<b> U.S.A</b> or in a serial or model number like <b>v1.2-a</b> they 
      will be treated as part of the word rather than as separate punctuation.</p>
    <p> The following table gives examples of character strings that the tokenizer 
      recognizes as terms. The exact specification is somewhat complex; for details 
      see <b> XTFTokenizer.jj</b> from the XTF Distribution (reproduced <a href=
        "Library/XTFTokenizer.jj.txt">here</a> for your convenience.)<br/>
    </p>

    <div class="Sample"> 
      <table cellpadding="0" cellspacing="12" width="617">
        <tr> 
          <td width="50%" align="right"> <span class="BaseStyle">Basic word: sequence 
            of digits and letters</span></td>
          <td width="61%"><span class="Code">boat &nbsp; Washington &nbsp; 6 &nbsp; 
            1895 &nbsp; Java2</span></td>
        </tr>
        <tr> 
          <td width="50%" align="right"> <span class="BaseStyle">Words with internal 
            apostrophes</span></td>
          <td width="61%"><span class="Code">O'Reilly &nbsp; you're &nbsp; O'Reilly's</span></td>
        </tr>
        <tr> 
          <td width="50%" align="right"> <span class="BaseStyle">Acronyms <i>(internal 
            dots are removed) </i></span></td>
          <td width="61%"><span class="Code">U.S.A &nbsp; I.B.M.</span></td>
        </tr>
        <tr> 
          <td width="50%" align="right"> <span class="BaseStyle">Company names</span></td>
          <td width="61%"><span class="Code">AT&amp;T &nbsp; Excite@Home</span></td>
        </tr>
        <tr> 
          <td width="50%" align="right"> <span class="BaseStyle">Email addresses</span></td>
          <td width="61%"><span class="Code">wer@all-one.com</span></td>
        </tr>
        <tr> 
          <td width="50%" align="right"> <span class="BaseStyle">Computer host 
            names</span></td>
          <td width="61%"><span class="Code">texts.cdlib.org &nbsp; main.server</span></td>
        </tr>
        <tr> 
          <td width="50%" align="right"> <span class="BaseStyle">Floating-point 
            numbers</span></td>
          <td width="61%"><span class="Code">3.14159 &nbsp; .6 &nbsp; 7,23</span></td>
        </tr>
        <tr> 
          <td width="50%" align="right"> <span class="BaseStyle">Dates</span></td>
          <td width="61%"><span class="Code">12/3/89 &nbsp; 3-Jan-02</span></td>
        </tr>
        <tr> 
          <td width="50%" align="right"> <span class="BaseStyle">Serial and model 
            numbers</span></td>
          <td width="61%"><span class="Code">12-6A &nbsp; 127.0.1.1 &nbsp; 270_ES 
            &nbsp; FX/7</span></td>
        </tr>
      </table>
    </div>
    
    <br/>
    After tokenizing, all upper-case letters within tokens are converted to lower-case, 
    which allows queries to be case-insensitive. Optional processing on tokens 
    can remove distinctions of plural vs. singular, and can remove diacritic accent 
    characters. </div>
</div>

<div class="IndentL"> 
  <p class="Heading2">Proximity and "Slop"<a name="Documents_Proximity"></a></p>
  <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
  As mentioned earlier, the inverted index not only maintains a list of the documents 
  each term appears in, but also each term's position. This information is then 
  used to support proximity-based queries, for example, searching for a pair of 
  terms within 10 words of each other. In <b><i>XTF</i></b>, proximity queries 
  are viewed as a "sloppy" match, and thus are specified in terms of a "slop" 
  value.<br/>
  <br/>
  <b>XTF Definition:</b> <i>slop</i>, n. The sum, for each term in the query, 
  of the distance between its position in the query and its position (relative 
  to the start of the match) in the source text. Note that slop is similar to 
  "edit distance" in Computer Science, but slop is easier to compute.<br/>
  <br/>
  For example, consider the query <span class="Code"> man <i>NEAR</i> war </span> 
  compared to this sentence in a document: "The <u>man went to war</u>." The potential 
  match will be on the words "man went to war". In this case, "man" is at position 
  1 in both, and thus contributes nothing to the slop. However, "war" is at position 
  2 in the query but position 4 in the match; thus it contributes 2 to the slop, 
  making the total slop 2. Thus for this to be considered a match, the proximity 
  query would have to specify a slop of 2 or greater. This can be summarized in 
  a table:<br/>
  <br/>
  <div class="Sample"> 
    <table class="BaseStyle">
      <tr align="center"> 
        <td>&nbsp;<b><u>Term</u></b>&nbsp;</td>
        <td>&nbsp;<b><u>Position in Query</u></b>&nbsp;</td>
        <td>&nbsp;<b><u>Position in Text</u></b>&nbsp;</td>
        <td>&nbsp;<b><u>Difference</u></b>&nbsp;</td>
      </tr>
      <tr align="center"> 
        <td align="center">&nbsp;man&nbsp;</td>
        <td align="center">&nbsp;1&nbsp;</td>
        <td align="center">&nbsp;1&nbsp;</td>
        <td align="center">&nbsp;0&nbsp;</td>
      </tr>
      <tr align="center"> 
        <td align="center">&nbsp;war&nbsp;</td>
        <td align="center">&nbsp;2&nbsp;</td>
        <td align="center">&nbsp;4&nbsp;</td>
        <td align="center">&nbsp;2&nbsp;</td>
      </tr>
      <tr align="center"> 
        <td></td>
        <td></td>
        <td>&nbsp;<b><i>total slop</i></b>&nbsp;</td>
        <td>&nbsp;<b><i>2</i></b>&nbsp;</td>
      </tr>
    </table>
  </div>
  <br/>
  The definition of slop penalizes terms that are found out-of-order in the document 
  text. So for example, consider a query for <span class="Code"> dog <i>NEAR</i> 
  house </span> compared to the sentence "Looking at his <u>house, our dog</u> 
  despaired." Even though there's only one word between the terms, the slop is 
  actually 3:<br/>
  <br/>
  <div class="Sample"> 
    <table class="BaseStyle">
      <tr align="center"> 
        <td>&nbsp;<b><u>Term</u></b>&nbsp;</td>
        <td>&nbsp;<b><u>Position in Query</u></b>&nbsp;</td>
        <td>&nbsp;<b><u>Position in Text</u></b>&nbsp;</td>
        <td>&nbsp;<b><u>Difference</u></b>&nbsp;</td>
      </tr>
      <tr align="center"> 
        <td align="center">&nbsp;dog&nbsp;</td>
        <td align="center">&nbsp;1&nbsp;</td>
        <td align="center">&nbsp;3&nbsp;</td>
        <td align="center">&nbsp;2&nbsp;</td>
      </tr>
      <tr align="center"> 
        <td align="center">&nbsp;house&nbsp;</td>
        <td align="center">&nbsp;2&nbsp;</td>
        <td align="center">&nbsp;1&nbsp;</td>
        <td align="center">&nbsp;1&nbsp;</td>
      </tr>
      <tr align="center"> 
        <td></td>
        <td></td>
        <td>&nbsp;<b><i>total slop</i></b>&nbsp;</td>
        <td>&nbsp;<b><i>3</i></b>&nbsp;</td>
      </tr>
    </table>
  </div>
  <br/>
  It is easy to see that if the slop were zero, then all the words must appear 
  in the source document in the same order with no intervening terms. This is 
  considered an exact match, generally referred to in this document as a "phrase". 
  <br/>
  <br/>
  <b>XTF Definition:</b> <i>phrase</i>, n. A proximity query with slop equal to 
  zero.<br/>
  <br/>
  When processing the source document, the <b><i>textIndexer</i></b> increments 
  the position for each term it encounters in the source document. However, at 
  sentence boundaries it incremented by five (this default can be changed), which 
  gives the effect of penalizing matches that cross sentence boundaries. Additionally, 
  special index pre-filter tags can increase the position (see the <a href=
        "tagRef.html#textIndexer_Tag_Ref">XTF Programming 
  Guide</a> for details.) 
  <p class="Heading2">Chunking<a name="Documents_Chunking"></a></p>
  <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
  The <i><b>textIndexer</b></i> tool splits source documents into smaller chunks 
  of text when adding the document to its index. Doing so makes proximity searches 
  and calculating their resulting summary blurbs or "snippets" faster by limiting 
  how much of the source document must be read into memory.<br/>
  <br/>
  <b>XTF Definition: </b><i>snippet</i>, n. A section of source text surrounding 
  and including a match (or "hit").<br/>
  <br/>
  The Lucene search engine forms the foundation of <b><i>XTF</i></b>'s search 
  capabilities. From Lucene's point of view, each of these text chunks is a single 
  searchable unit, independent of all other chunks. You might wonder then, "How 
  are proximity searches performed that span the boundary between two chunks?" 
  For instance, consider the following two chunks representing the sentence "The 
  quick brown fox jumped over the lazy dog."<br/>
  <br/>
  <div class="Sample"> 
    <table class="BaseStyle">
      <tr> 
        <td><b>Chunk 1:</b>&nbsp;</td>
        <td>the</td>
        <td>quick</td>
        <td>brown</td>
        <td>fox</td>
        <td>jumped</td>
      </tr>
      <tr align="center"> 
        <td><b>Chunk 2:</b>&nbsp;</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td>over</td>
        <td>the</td>
        <td>lazy</td>
        <td>dog</td>
      </tr>
    </table>
  </div>
  <br/>
  If one searched for the phrase "fox jumped over", the text engine would return 
  zero results. This is clearly unacceptable.<br/>
  <br/>
  The answer is that each chunk overlaps the previous one by a certain number 
  of terms, called the "chunk overlap". This overlapping area allows a match in 
  one chunk to extend into the next chunk by the overlapping number of words. 
  Thus the overlap limits and is equal to the maximum proximity the system can 
  handle. <br/>
  <br/>
  <div class="Sample"> 
    <table class="BaseStyle">
      <tr> 
        <td><b>Chunk 1:</b>&nbsp;</td>
        <td>the</td>
        <td>quick</td>
        <td>brown</td>
        <td>fox</td>
        <td>jumped</td>
      </tr>
      <tr align="center"> 
        <td><b>Chunk 2:</b>&nbsp;</td>
        <td></td>
        <td></td>
        <td>brown</td>
        <td>fox</td>
        <td>jumped</td>
        <td>over</td>
        <td>the</td>
      </tr>
      <tr align="center"> 
        <td><b>Chunk 3:</b>&nbsp;</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td>jumped</td>
        <td>over</td>
        <td>the</td>
        <td>lazy</td>
        <td>dog</td>
      </tr>
    </table>
  </div>
  <br/>
  The chunk size and chunk overlap are both configurable. If the selected chunk 
  overlap is large relative to the chunk size, space and processing time will 
  be wasted because many more chunks will be created. Conversely, making the overlap 
  very small limits the effective maximum "slop" value for all proximity queries. 
  Selecting these values is a trade-off between performance and maximum proximity. 
  <br/>
  <br/>
  The default values in the <b><i>XTF</i></b> distribution define a chunk size 
  of 200 words and an overlap of 20 words. These seem to give an adequate maximum 
  proximity, while minimizing processing time and disk space. One final note: 
  chunking is not performed on meta-data fields, as they are assumed to be relatively 
  small in size.<br/>
  <p class="Heading2">Stop Words and Bi-grams<a name="Documents_StopWords"></a></p>
  <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
  Recall that <b><i>XTF</i></b> builds an <i>inverted index</i> using Lucene. 
  For each term found in any document, the index stores a list of each occurrence 
  of that term.<br/>
  <br/>
  Now consider a term like "the". It occurs so commonly in English-language texts 
  that the list of all occurrences becomes very large and thus takes a long time 
  to process. Common words like "the" are called stop words; other examples are 
  "a", "an", "it", "and", "in", "is".<br/>
  <br/>
  A key observation is that these stop words, because they're so common, are uninteresting 
  to search for. One's initial tendency might be then to simply ignore them, and 
  this solution indeed speeds up searching.<br/>
  <br/>
  <b>XTF Definition: </b><i>stop word</i>, n. A very common word that is generally
  uninteresting to search for.<br/>
  <br/>
  For instance, a search for "man in war" would be interpreted as "man war". Unfortunately, 
  this will turn up occurrences of "man of war" (a kind of jellyfish, and not 
  what the user intended.)<br/>
  <br/>
  So the second key observation is that stop words are useful <i>in conjunction 
  with non-stop words</i>. While "in" is a very common word, the combination "man-in" 
  is much less common, and is thus much faster to search for. This leads us directly 
  to the idea of bi-grams, which <b><i>XTF</i></b> implements to get almost the 
  speed of eliminating stop words, but still providing good query results.<br/>
  <br/>
  <b>XTF Definition: </b><i>bi-gram</i>, n. A single term in the index, composed 
  of a stop word and a non-stop word fused together.<br/>
  <br/>
  Consider the sentence "A friend in need is a friend indeed." Scanning for stop- 
  words and combining them with adjacent words, we get the following sequence 
  of terms (regular terms are marked in <b>bold</b>, while bi-grams are <u>underlined</u
        >):<br/>
  <br/>
  <div class="Sample"> Index: <u>a-friend</u> &nbsp;<b>friend</b> &nbsp;<u>friend-in</u> 
    &nbsp;<u>in-need</u> &nbsp;<b>need</b> &nbsp;<u>need-is</u> &nbsp;<u>is-a</u> 
    &nbsp;<u>a-friend</u> &nbsp;<b>indeed</b> </div>
  <br/>
  As you can see, the index is quite different when bi-grams are added into it. Consequently, 
  a similar transformation must be performed when a query is made, essentially 
  rewriting the query. For instance, a search for a <i>phrase</i> "<b>friend</b> 
  in <b>need</b>" is re-written to search for the phrase "<u>friend-in</u> <u>in-need</u>".<br/>
  <br/>
  More complex transformations are required for NEAR queries. For instance, consider 
  the proximity query: "<b>friend</b> NEAR <b>in</b> NEAR <b>need</b>". The engine 
  rewrites the query with and without stop words, so that if any exact matches 
  are found, they will be ranked higher, but if not, any matches containing "friend" 
  near "need" will be found. The resulting query looks like this:<br/>
  <br/>
  <div class="Sample"> Query: (<b>friend</b> OR <u>friend-in</u>) NEAR (<u>in-need</u> 
    OR <b>need</b>) </div>
  <br/>
  In summary, transforming stop words into bi-grams speeds up query processing, 
  while retaining the ability to include stop words in a query. <br/>
  <!-- IndentL --> 
  <p class="Heading1">Query Operations and What They Do<a name="QueryOps"></a></p>
  <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
  This section gives details on how queries are interpreted, and specifies how 
  the various query operators work. Note that meta-data and text queries are treated 
  somewhat differently. This is due to the fact that meta-data fields are assumed 
  to be short, while the full text of a document is assumed to be very large. 
  <p class="Heading2">Interpreting User Queries<a name="QueryOps_User"></a></p>
  <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
  The job of translating queries from an input URL to a form that <b><i>XTF</i></b> 
  can understand is undertaken by the Query Parser stylesheet. The query parser 
  included in the base distribution is relatively simple: by default, it simply 
  forms an AND query consisting of all the input terms. All non-word terms (such 
  as the "++" in "C++") are ignored. Optionally, the operation can be changed 
  to OR or NEAR. In addition, terms can be excluded if desired.<br/>
  <br/>
  Here are some sample URLS, and how the default query parser interprets each 
  one: <br/>
  <br/>
  <div class="Sample"> <span class="Code"> http://<span class="MacroCode">yourserver.org</span>:8080/xtf/search?title=apartheid+mind 
    </span><br/>
    <div class="IndentLR"> Interpreted as: <span class="Code">title:(apartheid 
      AND mind)</span> <br/>
      <i>(Note that "<span class="Code">+</span>" is the URL coding for a space.)</i> 
    </div>
    <br/>
    <span class="Code"> http://<span class="MacroCode">yourserver.org</span>:8080/search?title=apartheid+mind&amp;title-exclude=mandela 
    </span><br/>
    <div class="IndentLR"> Interpreted as: <span class="Code">title:((apartheid 
      AND mind) NOT (mandela))</span> </div>
    <br/>
    <span class="Code"> http://<span class="MacroCode">yourserver.org</span>:8080/search?title=apartheid+mind&amp;title-join=or 
    </span><br/>
    <div class="IndentLR"> Interpreted as: <span class="Code">title:(apartheid 
      OR mind)</span> </div>
    <br/>
    <span class="Code"> http://<span class="MacroCode">yourserver.org</span>:8080/search?title=apartheid&amp;text=mandela 
    </span><br/>
    <div class="IndentLR"> Interpreted as: <span class="Code">(title:apartheid) 
      AND (text:mandela)</span> </div>
    <br/>
    <span class="Code"> http://<span class="MacroCode">yourserver.org</span>:8080/search?text=%22Nelson+Mandela%22&amp;subject=africa 
    </span><br/>
    <div class="IndentLR"> Interpreted as: <span class="Code">(text:PHRASE "nelson 
      mandela") AND (subject:africa)</span> <br/>
      <i>(Note that "<span class="Code">%22</span>" is the URL coding for a quote 
      character.)</i> </div>
    <br/>
    <span class="Code"> http://<span class="MacroCode">yourserver.org</span>:8080/search?text=Mandela+Apartheid&amp;text-join=5 
    </span><br/>
    <div class="IndentLR"> Interpreted as: <span class="Code">(text:Mandela NEAR{proximity=5} 
      Apartheid)</span> </div>
  </div>
  <br/>
  Of course much more complex query parsing is possible, since the <b>Text Engine</b> 
  can handle arbitrarily complex queries consisting any combination of boolean 
  query operators. Creating such a system is, however, left to the system designer 
  setting up <b><i>XTF</i></b>, as it intermeshes closely with whatever HTML form 
  or other mechanism is used to input the query, and is highly dependent on the 
  skill level and needs of the final users of the system.<br/>
  <br/>
  <p class="Heading2">Text Query Operations<a name="QueryOps_Text"></a></p>
  <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
  <b><i>XTF</i></b> implements a full complement of "boolean" operators used to 
  form complex queries: AND, OR, NEAR, PHRASE, RANGE, and NOT, and supports wildcard 
  search characters. This section covers the details of how these queries are 
  interpreted within the very large documents <b><i>XTF</i></b> can handle. <br/>
  <br/>
  <div class="IndentLR"> 
    <p class="Heading3">TERM and Wildcard Queries on Text<a name="QueryOps_Text_TERMWILD"></a></p>
    <br/>
    <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
    <div class="IndentL"> A <b>TERM</b> query matches every occurrence of the 
      specified term in the document. Upper-case vs. lower-case distinctions are 
      ignored. Additionally, the term may contain special wildcard characters:<br/>
      <br/>
      <div class="IndentL"> 
      <table class="BaseStyle">
        <tr>
          <td valign="top"><span class="Code">?</span> &nbsp;</td>
          <td valign="top">The question-mark character matches terms with any character at
            that position. For example, <span class="Code">lo?e</span> would match "love"
              <i>or</i> "lose", but <i>not</i> "loe". </td>
        </tr>
        <tr>
          <td valign="top"><span class="Code">*</span> &nbsp;</td>
          <td valign="top">The asterisk character matches terms with any number
            (including zero) characters at that position. For example, <span class="Code"
            >dog*</span> would match any of the following terms: "dog", "dogs", "doggie",
            "doggerel", etc. </td>
        </tr>
      </table>
      </div>
      <br/>
      Depending on the particular wildcard, hundreds or even thousands of terms 
      might match, so care should be taken when using these. To avoid allowing 
      such queries to occupy the engine for long periods of time, <b><i>XTF</i></b> 
      allows queries to specify a limit on the maximum number of terms to match
      (controlled by the 
      <a href="tagRef.html#crossQuery_QueryParser_Output_Query"><span class="Code">workLimit</span></a> attribute.) 
      Queries that exceed the limit produce an error.<br/>
      <br/>
    </div>
    <p class="Heading3">AND Query on Text<a name="QueryOps_Text_AND"></a></p>
    <br/>
    <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
    <div class="IndentL"> What does it mean to search for "man AND war" in the 
      full text of all documents? Perhaps the most obvious answer would be to 
      search for any document containing both words. But consider a document where 
      "man" appeared in Chapter 1 and "war" appeared in Chapter 7. Would that 
      be a document the user really wanted to find? Probably not. More likely 
      they'd be interested in a document where "man" and "war" appear <i>close 
      together</i>.<br/>
      <br/>
      Thus <b><i>XTF</i></b> interprets <b>AND</b> queries on the full text as 
      <b> NEAR</b> queries instead, with the slop factor set to the maximum for 
      that index. <br/>
      <br/>
      More formally, when used with terms, the <b>AND</b> query will match any 
      section of text that contains <i>all</i> of the terms, in any order, as 
      long as they are close together (that is, within the maximum proximity defined 
      for the index, or 20 words in the default configuration.)<br/>
      <br/>
      When used to group sub-queries together, <b>AND</b> will match text where 
      all of the sub-queries match, in any order, as long as the matches are close 
      together (i.e. within the maximum proximity for the index.) </div>
    <p class="Heading3">OR Query on Text<a name="QueryOps_Text_OR"></a></p>
    <br/>
    <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
    <div class="IndentL"> When used to group terms, an <b>OR</b> query matches 
      each occurrence of every term contained within it.<br/>
      <br/>
      If used to group sub-queries together, the <b>OR</b> query matches each 
      occurrence of every sub-query. </div>
    <p class="Heading3">PHRASE Query on Text<a name="QueryOps_Text_PHRASE"></a></p>
    <br/>
    <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
    <div class="IndentL"> A <b>PHRASE</b> query generally contains two or more 
      terms, and it matches any span of text where all the terms appear together, 
      in order, with no other terms between them.<br/>
      <br/>
      Less frequently, it can be used to group sub-queries. It matches a span 
      of text where all of the sub-queries match, in order, without any intervening 
      non-matching terms.<br/>
      <br/>
      Note that a <b>PHRASE</b> query is equivalent to a <b>NEAR</b> query with 
      a slop factor of zero. </div>
    <p class="Heading3">NEAR Query on Text<a name="QueryOps_Text_NEAR"></a></p>
    <br/>
    <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
    <div class="IndentL"> Each <b>NEAR</b> query requires a "slop" factor. In 
      rough terms, this factor can be thought of as limiting the amount of sloppiness 
      when matching. A slop of zero indicates very tight control; in fact, a <b>NEAR</b> 
      query with zero slop is equivalent to a <b>PHRASE</b> query. A large slop 
      value indicates that terms may appear far apart, or out of order, or both. 
      Note however that the slop value is silently constrained to the maximum 
      proximity defined by the chunk overlap of an index.<br/>
      <br/>
      For more details on how slop is computed, see the section on <a href=
            "#Documents_Proximity">Proximity and Slop</a>. For information on 
      chunk overlap and how it relates to proximity searching, see the section 
      on <a href=
            "#Documents_Chunking">Chunking</a>.<br/>
      <br/>
      The <b>NEAR</b> query, when used with terms, matches a span of text containing 
      all of the terms, where the match's slop is less than or equal to the slop 
      factor specified for the query.<br/>
      <br/>
      When used to group sub-queries, it matches a span of text where all of the 
      sub-queries match, and the complete match's slop is less than or equal to 
      the slop factor specified for the <b>NEAR</b> query. </div>
    <p class="Heading3">NOT Clause on Text<a name="QueryOps_Text_NOT"></a></p>
    <br/>
    <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
    <div class="IndentL"> A <b>NOT</b> clause may be specified as a sub-query 
      of any boolean query (<b>OR</b>, <b>AND</b>, <b>PHRASE</b>, or <b>NEAR</b>). 
      Any matches in the <b>NOT </b> clause will suppress outer matches within 
      the maximum proximity of the index. This can be thought of as a "kill zone": 
      each match within the <b>NOT </b> clause kills off nearby matches. </div>
  </div>
  <p class="Heading2">Meta-data Query Operations<a name="QueryOps_Meta"></a></p>
  <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
  The following query operations can be applied to any meta-data field. Queries 
  are applied to meta-data and full text in like fashion, with a few exceptions: 
  <b>AND</b> queries are not proximity-based in meta-data fields, <b>NOT</b> clauses 
  on meta-data can eliminate 
  whole documents, and a new operator, <b>RANGE</b>, is available on meta-data fields. 
  <div class="IndentLR"> 
    <p class="Heading3">TERM and Wildcard Queries on Meta-data<a name="QueryOps_Meta_TERMWILD"></a></p>
    <br/>
    <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
    <div class="IndentL"> A <b>TERM</b> query matches every occurrence of the 
      specified term in the meta-data field. Upper-case vs. lower-case distinctions 
      are ignored. Additionally, the term may contain special wildcard 
      characters:<br/>
      <br/>
      <div class="IndentL"> 
        <table class="BaseStyle">
          <tr> 
            <td valign="top"><span class="Code">?</span> &nbsp;</td>
            <td valign="top">The question-mark character matches terms with any 
              character at that position. For example, <span class="Code">lo?e</span> 
              would match "love" <i>or</i> "lose", but <i>not</i> "loe". </td>
          </tr>
          <tr>
            <td valign="top"><span class="Code">*</span> &nbsp;</td>
            <td valign="top">The asterisk character matches terms with any 
              number (including zero) characters at that position. For example, 
              <span class="Code">dog*</span> would match any of the following terms: 
              "dog", "dogs", "doggie", "doggerel", etc. </td>
          </tr>
        </table>
      </div>
      <br/>
      Depending on the particular wildcard, hundreds or even thousands of terms 
      might match, so care should be taken when using these. To avoid allowing 
      such queries to occupy the engine for long periods of time, <b><i>XTF</i></b> 
      allows queries to specify a limit on the maximum number of terms to match
      (controlled by the 
      <a href="tagRef.html#crossQuery_QueryParser_Output_Query"><span class="Code">workLimit</span></a> attribute.) 
      Queries that exceed the limit produce an error.<br/>
      <br/>
    </div>
    <p class="Heading3">RANGE Queries on Meta-data<a name="QueryOps_Meta_RANGE"></a></p>
    <br/>
    <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
    <div class="IndentL"> A <b>RANGE</b> query is similar to a wildcard term query 
      in that it matches a (possibly large) number of terms. Lower and upper bounding 
      terms are specified, and <i>every</i> term that appears in the index lexicographically 
      between the two bounds is matched.<br/>
      <br/>
      For example, if the lower bound were "1895" and the upper bound were "1900", 
      a range query would match any of the terms 1895, 1896, 1897, 1898, 1899, 
      and 1900. Optionally, the query can exclude the bounds, in which case it 
      wouldn't match 1895 nor 1900.<br/>
      <br/>
      As in the case of wildcard queries, care must be taken to avoid searching 
      a huge number of terms. This can happen easily: in the case of the example 
      above, if dates were encoded in the index in the form "YYYY-MM-DD", then 
      all the days from 1895 to 1900 would match... potentially 2,190 of them. 
      And of course a range query from A to Z would match practically every term 
      in the index. Again, each query can specify a limit on the maximum number 
      of terms to match, to avoid bogging down the engine. <br/>
      <br/>
      However, when searching numeric data (for example, file time and date stamps)
      the above wildcard approach simply is not sufficient. To handle this, 
      XTF provides a special <i>numeric</i> range searching capability. This is
      specified as an attribute to the normal <span class="Code">&lt;range></span>
      query operator, but it tells XTF that the data is numeric, and in a
      rigid format (such as YYYY-MM-DD:HH-MM-SS; any rigid format is acceptable).
      When the first such query is made, XTF loads a table of all the data values
      and converts them to 64-bit integers. This table is then cached in memory,
      and range queries on that field are processed very quickly, avoiding any
      wildcard-like expansion.
    </div>
    <p class="Heading3">AND Query on Meta-data<a name="QueryOps_Meta_AND"></a></p>
    <br/>
    <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
    <div class="IndentL"> Unlike in full-text queries, an <b>AND</b> query on 
      meta-data implies no proximity restrictions. When used with terms, it matches 
      documents where <i>every</i> term appears somewhere in the field, in any 
      order.<br/>
      <br/>
      When used to group sub-queries, it matches documents where <i>all</i> of 
      the sub-queries match (note that the sub-queries may be on several different 
      fields.) </div>
    <p class="Heading3">OR Query on Meta-data<a name="QueryOps_Meta_OR"></a></p>
    <br/>
    <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
    <div class="IndentL"> When used to group terms, an <b>OR</b> query matches 
      a document where any of the terms occurs within the meta-data field.<br/>
      <br/>
      If used to group sub-queries together, the <b>OR</b> query matches documents 
      that match by <i>any</i> of the sub-queries (note that the sub-queries may 
      involve several different fields.) </div>
    <p class="Heading3">PHRASE Query on Meta-data<a name="QueryOps_Meta_PHRASE"></a></p>
    <br/>
    <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
    <div class="IndentL"> A <b>PHRASE</b> query generally contains two or more 
      terms, and it matches any document where the terms appear together in the 
      field, in order, with no other terms between them.<br/>
      <br/>
      Less frequently, it can be used to group sub-queries. It matches any document 
      where all of the sub-queries match, in order, without any intervening non-matching 
      terms.<br/>
      <br/>
      Note that a <b>PHRASE</b> query is equivalent to a <b>NEAR</b> query with 
      a slop factor of zero. </div>
    <p class="Heading3">NEAR Query on Meta-data<a name="QueryOps_Meta_NEAR"></a></p>
    <br/>
    <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
    <div class="IndentL"> Each <b>NEAR</b> query requires a "slop" factor. In 
      rough terms, this factor can be thought of as limiting the amount of sloppiness 
      when matching. A slop of zero indicates very tight control; in fact, a <b>NEAR</b> 
      query with zero slop is equivalent to a <b>PHRASE</b> query. A large slop 
      value indicates that terms (or sub-queries) may appear far apart, or out 
      of order, or both. There is no upper bound on the slop factor. For more 
      details on how slop is computed, see the section on <a href=
            "#Documents_Proximity">Proximity and Slop</a>. The <b>NEAR</b> query, 
      when used with terms, matches any document where all of the terms appear 
      in the field and their group slop is less than or equal to the slop factor 
      specified for the query.<br/>
      <br/>
      When used to group sub-queries, it matches any document where all of the 
      sub-queries match, and the complete match's slop is less than or equal to 
      the slop factor specified for the <b>NEAR</b> query. </div>
    <p class="Heading3">NOT Clause on Meta-data<a name="QueryOps_Meta_NOT"></a></p>
    <br/>
    <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
    <div class="IndentL"> A <b>NOT</b> clause may be specified as a sub-query 
      of any boolean query (<b>OR</b>, <b>AND</b>, <b>PHRASE</b>, or <b>NEAR</b>). 
      Any documents matching the <b>NEAR</b> clause will be removed from the outer 
      set of matches. </div>
  </div>
  <!-- IndentL --> 
  <p class="Heading2">Stop Words in Queries<a name="QueryOps_StopWords"></a></p>
  <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
  If a query contains one or more stop words, the query will be internally rewritten 
  to work properly with the bi-gram system. Recall from the section on <a href=
        "#Documents_StopWords">Stop Words and Bi-grams</a> that using bi-grams allows 
  <b><i> XTF</i></b> to support queries containing stop words while avoiding the 
  usual severe impact on performance that they might have.<br/>
  <br/>
  Here are some details on how stop words are handled in various query situations:<br/>
  <ul>
    <li> In the absence of any grouping operator, querying for a single stop word 
      in a <b>TERM</b> query will produce an empty result set. </li>
    <li> Wildcards queries will skip stop words; for example, searching for "th?" 
      would not match "the" (which is a stop word), but would still match "thy". 
    </li>
    <li> Stop words are stripped out of <b>OR</b> queries and <b>NOT</b> clauses. 
    </li>
    <li> By contrast, <b>PHRASE</b> queries retain all stop words, because users 
      would be dismayed if a query on "man of war" returned matches on "man in 
      war". </li>
    <li> In <b>AND</b> and <b>NEAR</b> queries, stop words are joined with adjacent 
      words to form bi-grams. The resulting query will effectively prefer matches 
      containing the stop words in the correct places, but will allow matches 
      where they don't appear. </li>
  </ul>
  <!-- IndentL --> 
  <p class="Heading1">Term and Hit Marking<a name="Marking"></a></p>
  <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
  When a meta-data field, snippet, or marked up document is fed into a <b><i> 
  crossQuery</i></b> or <b><i>dynaXML</i></b> formatting stylesheet, <b><i>XTF</i></b> 
  inserts XML tags to indicate matched terms in context, and also indicates the 
  extent of full-text matches, also called "text hits". <br/>
  <br/>
  <b>XTF Definition:</b> <i>text hit</i>, n. A consecutive span of words in a 
  document that matches a text query. (Note that there may be many hits per document.)<br/>
  <br/>
  This section covers details of which terms are marked where, what "snippets" 
  are and how they are formed, and how hits are marked, both within snippets and 
  in their original context.<br/>
  <br/>
  <div class="IndentL"> 
    <p class="Heading2">Snippet Formation and Marking<a name="Marking_Snippets"></a></p>
    <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
    Hits found in the full text of a document are, of course, surrounded by other 
    text that can help the user decide if the hit is useful to them. <b><i>XTF</i></b> 
    provides this context by calculating a "snippet" for each hit within a document. 
    <br/>
    <br/>
    <b>XTF Definition: </b><i>snippet</i>, n. A section of the source text or 
    a meta-data field, surrounding and including a match (or "hit").<br/>
    <br/>
    A query may specify the optimal length (in characters) of a snippet, and the 
    system will get as close as it can to that length without exceeding it. The 
    default is 80 characters.<br/>
    <br/>
    The process used to form snippets is fairly simple:<br/>
    <ul>
      <li>First, the <b>Text Engine</b> locates the matching text and surrounds 
        it with a <span class="Code">&lt;hit&gt;</span> tag.</li>
      <li>Next, the engine adds words found in the source document before and 
        after the hit, until it cannot add any more without exceeding the specified 
        snippet size. It attempts to equalize the amount of context added before 
        vs. after the hit.</li>
      <li>Finally, each matching term is marked with a <span class="Code">&lt;term&gt;</span> 
        tag. </li>
    </ul>
    Note that many hits will contain terms that aren't part of the query and thus 
    won't be marked with <span class="Code">&lt;term&gt;</span>. For instance, 
    a search for "dog NEAR skeleton NEAR bone" on the text "The dog chewed on 
    the skeleton's leg bone." would yield:<br/>
    <br/>
    <div class="Sample"> <span class="CodeNorm">The &lt;hit&gt;<b>&lt;term&gt;</b>dog<b>&lt;/term&gt;</b> 
      chewed on the<br/>
      <b>&lt;term&gt;</b>skeleton's<b>&lt;/term&gt;</b> leg <b>&lt;term&gt;</b>bone<b>&lt;/term&gt;</b>&lt;/hit&gt;</span> 
    </div>
    <br/>
    Snippets are provided to the <b>Document Formatter</b> stylesheet in <b><i>crossQuery</i></b> 
    for displaying a summary of hits in all documents, and to the <b>Document 
    Formatter</b> stylesheet in <b><i>dynaXML</i></b> to display a ranked list 
    of snippets in a single document. 
    <p class="Heading2">Hits in their Original Context<a name="Marking_Context"></a></p>
    <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
    In addition to snippets, <b><i>dynaXML</i></b> also inserts <span class="Code">&lt;hit&gt;</span> 
    tags and <span class="Code">&lt;term&gt;</span> tags into the original XML 
    document before feeding it to the <b>Document Formatter</b> stylesheet. This 
    allows the stylesheet to display the document contents with the hits and terms 
    highlighted in their original context.<br/>
    <br/>
    The tags are identical to those contained in snippets (except of course that 
    surrounding text need not be inserted, as the hits are marked in the original 
    context.) Additional attributes are added to each hit, giving its score, rank, 
    and hit number.<br/>
    <br/>
    Note that query terms are marked everywhere they appear in the document text, 
    not just within <span class="Code">&lt;hit&gt;</span> markers. This gives 
    the stylesheet the option of highlighting them inside and/or outside hits. 
    <p class="Heading2">Spanning XML Tags<a name="Marking_Span"></a></p>
    <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
    Complication arises when a hit crosses the boundary between two XML elements 
    in the original source text. <b><i>XTF</i></b> takes care not to alter the 
    structure of the document, so hit tags in this circumstance are inserted in 
    a special way. Essentially, the hit is divided into several stretches of unbroken 
    text. The first stretch is marked with an <span class="Code">&lt;xtf:hit&gt;</span> 
    tag with its <span class="Code">continues</span> attribute set to "yes". The 
    subsequent stretches are each marked with an <span class="Code">&lt;xtf:more&gt;</span> 
    tag; the <span class="Code">continues</span> attribute for each is "yes" except 
    for the last, which is "no".<br/>
    <br/>
    The goal is to allow the <b>Document Formatter</b> stylesheet to present a 
    seamless interface to the end-user, who is probably unaware of the underlying 
    structure of a document and is only concerned with where the hits fall.<br/>
    <br/>
    For example, say we had the following source text containing <span class="Code">&lt;i&gt;</span> 
    to mark an italicized section:<br/>
    <br/>
    <div class="Sample"> <span class="Code">The hungry plant yearned for &lt;i&gt;man-flesh&lt;/i&gt; 
      to fill its bottomless gullet.</span> </div>
    <br/>
    If the user searched for "plant NEAR man" they would expect results something 
    like this:<br/>
    <br/>
    <div class="Sample"> The hungry <span class="YellowBG"><span class="RedB">plant</span> 
      yearned for </span><i><span class="YellowBG"><span class="RedB">man</span></span>-flesh</i> 
      to fill its bottomless gullet. </div>
    <br/>
    To support this sort of behavior, <b><i>XTF</i></b> would mark up the source 
    document like this (broken into multiple lines and indented for clarity):<br/>
    <br/>
    <div class="Sample"> 
      <pre class="CodeNorm">The hungry
<b>&lt;hit hitNum="1" continues="yes"&gt;</b>
  &lt;term&gt;plant&lt;/term&gt; yearned for
<b>&lt;/hit&gt;</b>
&lt;i&gt;
  <b>&lt;more hitNum="1" continues="no"&gt;</b>
    &lt;term&gt;man&lt;/term&gt;
  <b>&lt;/more&gt;</b>-flesh
&lt;/i&gt;
to fill its bottomless gullet.</pre>
      </div>
    <br/>
    Let's consider another example. If the user searched for "plant NEAR bottomless", 
    the resulting hit would completely span across the <span class="Code">&lt;i&gt;</span> 
    tag. Reasonable results from the <b>Document Formatter</b> could be:<br/>
    <br/>
    <div class="Sample"> The hungry <span class="YellowBG"><span class="RedB">plant</span> 
      yearned for <i>man-flesh</i> to fill its <span class="RedB">bottomless</span></span> 
      gullet. </div>
    <br/>
    Here is how <b><i>XTF</i></b> would mark up this example:<br/>
    <br/>
    <div class="Sample">
      <pre class="CodeNorm">The hungry
<b>&lt;hit hitNum="1" continues="yes"&gt;</b>
  &lt;term&gt;plant&lt;/term&gt; yearned for
<b>&lt;/hit&gt;</b>
&lt;i&gt;
  <b>&lt;more hitNum="1" continues="yes"&gt;</b>
    man-flesh
  <b>&lt;/more&gt;</b>
&lt;/i&gt;
<b>&lt;more hitNum="1" continues="no"&gt;</b>
  to fill its &lt;term&gt;bottomless&lt;/term&gt;
<b>&lt;/more&gt;</b> gullet.</pre>
      </div>
    <br/>
    <p class="Heading2">Special Rules for Marking Stop Words<a name="Marking_Stopwords"></a></p>
    <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
    When stop words are part of the query, special rules are applied when marking 
    them in snippets and in their original document context:<br/>
    <br/>
    <ul>
      <li> Within a snippet, stop words are only marked with a <span class="Code">&lt;term&gt;</span> 
        tag when they appear within a <span class="Code">&lt;hit&gt;</span>, and 
        then only as part of an adjoining non-stop-word that actually contributed 
        to the match. </li>
      <li> Likewise, when the original XML document is marked up for the <b>Document 
        Formatter</b>, stop words are only marked within a <span class="Code">&lt;hit&gt;</span>, 
        and only as part of an adjoining term that contributed to the match. </li>
    </ul>
    <br/>
  </div>
  <!-- IndentL --> 
  <p class="Heading1">Hit Scoring<a name="Scoring"></a></p>
  <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
  <p><b><i>XTF</i></b> uses and extends Lucene's built-in scoring mechanism to provide 
  a relevance score for each hit, and to return hits in ranked order (i.e. highest 
  score first.) This section describes briefly how the <b>Text Engine</b> determines 
  the score for hits in the full text and hits on meta-data fields.
  </p>
  <p>
    You can observe the scoring engine in action by enabling the 
    <span class="Code">explainScores</span> attribute on the 
    <span class="Code">&lt;query></span> element produced by your
    <b>Query Parser</b> stylesheet. See the 
    <a href="tagRef.html#crossQuery_QueryParser_Output_Query">Tag Reference</a> for more
    information on how to enable this.
  </p>
  <p>
    The following sections break down XTF's scoring calculation like this:
    first we cover the common aspects shared by both meta-data and text
    chunk scoring, then talk about the differences, and finally take a look
    at how the final combined score for a document is computed.
  </p>
  <div class="IndentL"> 
    <p class="Heading2">Individual Hit Scoring<a name="Scoring_Hits"></a></p>
    <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
    <p>
      Whether a hit (another name for a single match) is in a meta-data field or 
      within the full text of
      a document, the scoring for that particular hit is the same. How
      the scores are combined differs, and those differences are covered
      in later sections.
    </p>
    <p>
      For those intimately familiar with Lucene, it will be helpful to know that
      <span class="Keyword">XTF</span> makes extensive use of Lucene's
      "span" queries, to enable the exact identification of particular matches
      within a large document. XTF's implementation of spans includes enhancements
      that calculate the score of each span in addition to its "slop".
    </p>
    <p>
      Queries on the contents of an <span class="Keyword">XTF</span> index
      are scored using an enhanced version of Lucene's standard formula. 
      The structure of the scoring formula is fixed, but 
      one can override the calculation of the various factors by providing a Java 
      implementation of the <b>Similarity</b> interface. 
    </p>
    <div class="IndentL"> 
      <p class="Heading3">Plain English</p>
      <ul>
        <li> If a term appears many times in a field or chunk of a document, 
          the match will rank 
          higher; if it only appears a few times, the match will rank lower. 
        </li>
        <li> Rare terms are given more weight than common terms. </li>
        <li> Any field or section of text in the document can be boosted at index time; 
          hits in boosted sections will rank higher. </li>
        <li> For <b>AND</b> and <b>NEAR</b> queries, more exact matches will rank 
          higher than sloppy matches. That is, a hit where the terms appear in 
          order without intervening words will rank higher than an out-of-order 
          match with many other words interspersed. </li>
        <li> Matches in short fields are ranked higher than those in long fields. 
        </li>
        <li> In an <b>OR</b> query, hits that match many of the terms will 
          rank higher than those that only match a few. </li>
      </ul>
      A note on dampening: very old versions of XTF used "dampening" around
      high-ranking hits to avoid overlapping snippets. However, more recent
      versions automatically avoid overlapping hits regardless of their
      scores, so dampening is no longer performed.
      <p class="Heading3">Mathematical Details</p>
      <br/>
      For a given query <b>q</b>, the score for a matching span <b>s</b> consisting of terms <b>t</b>, 
      in field
      (or text chunk) <b>f</b> of document <b>d</b>, is calculated as follows: 
      <div class="IndentL"> 
        <table cellpadding="0" cellspacing="0" border="0">
          <tr> 
            <td valign="middle" align="right" rowspan="2">spanScore(q,s) = 
              sloppyFreq(s) * boost(f,d) * lengthNorm(f,d) * coord(q,s) * <br/>
            </td>
            <td valign="middle" align="center"> <big><big><big><big>&Sigma;</big></big></big></big></td>
            <td valign="middle"><small> idf(t) </small></td>
          </tr>
          <tr> 
            <td valign="top" align="right"> <small>t in s</small> </td>
          </tr>
        </table>
      </div>
      where 
      <ul>
        <li> <b>sloppyFreq(s)</b> is a factor that decreases as the 
          <a href="#Documents_Proximity">sloppiness</a> 
          of the matching span increases. The effect is to favor more exact matches. Default 
          implementation: 1 / (slop + 1). In the special case of <b>OR</b> queries, or
          <b>AND</b> queries where proximity has been disabled, slop
          is ignored and 1.0 is used instead.</li>
        <li> <b>boost(f,d)</b> is the boost factor (if any) applied to the field
          (or section of text) containing the match.</li>
        <li> <b>lengthNorm(f,d)</b> is a factor that decreases as the 
          amount of text in the field or text chunk increases,
          since matches in longer fields are generally less precise. Default implementation: 
          square root of the number of terms in the field/chunk. </li>
        <li> <b>coord(q,s)</b> is a factor based on the fraction of all query 
          terms that are matched by the span. Spans with a higher ratio of matching
          terms will be ranked higher.
          Default implementation: 
          number of terms matched / number of terms in query. </li>
        <li> <b>idf(t)</b> is a factor based on the number of documents or
          text chunks containing 
          <b>t</b>. Terms that occur in fewer documents/chunks are better indicators of 
          topic, so <b>idf</b> is high when <b>t</b> appears seldom in the index, and 
          low when <b>t</b> appears often in the index. Default implementation: log(number 
          of docs/chunks in index / number of docs/chunks containing <b>t</b>) </li>
      </ul>
    </div>
    <p class="Heading2">Text Hit Scoring<a name="Scoring_Text"></a></p>
    <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
    The full text of a document might contain thousands of individual matching
    spans, each of which will be scored according to the method above. How are
    these scores combined into a single score for the text?
    <div class="IndentL"> 
      <p class="Heading3">Plain English</p>
      <ul>
        <li> Documents with more matches will score higher than those with few matches.</li>
        <li> Matches in short texts are ranked higher than those in long texts. 
        </li>
      </ul>
      <p class="Heading3">Mathematical Details</p>
      <br/>
      For a given query <b>q</b>, the score for all matching spans <b>s</b> in all
      text chunks of document
      <b>d</b> is calculated as follows: 
      <div class="IndentL"> 
        <table cellpadding="0" cellspacing="0" border="0">
          <tr> 
            <td valign="middle" align="right" rowspan="2">textScore(q,d) = lengthNorm(d,text) *
              tf( <br/>
            </td>
            <td valign="middle" align="center"> <big><big><big><big>&Sigma;</big></big></big></big></td>
            <td valign="middle"><small> spanScore(q,s)</small></td>
            <td valign="middle" align="right" rowspan="2"> &nbsp;) <br/></td>
          </tr>
          <tr> 
            <td valign="top" align="right"> <small>s in d</small> </td>
          </tr>
        </table>
      </div>
      where 
      <ul>
        <li> <b>lengthNorm(d,text)</b> is a factor that decreases as the 
          amount of text in the document increases, since matches 
          in longer texts are generally less precise. Default implementation: 
          square root of the number of chunks in the document. </li>
        <li> <b>tf(...)</b> is a score factor that helps to equalize the scoring
          between documents with many matches and those with few matches.
          Default implementation: 
          square root of the total score of the matches in <b>d</b>. </li>
      </ul>
    </div>
    <p class="Heading2">Meta-data Hit Scoring<a name="Scoring_Meta"></a></p>
    <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
    The scores for multiple hits within a single meta-data field are
    combined in a similar manner to text hits, above.
    <div class="IndentL"> 
      <p class="Heading3">Plain English</p>
      <ul>
        <li> Fields with more matches will score higher than those with few matches.</li>
        <li> Matches in short fields are ranked higher than those in long fields. 
        </li>
      </ul>
      <p class="Heading3">Mathematical Details</p>
      <br/>
      For a given query <b>q</b>, the score for all matching spans <b>s</b> in field
      <b>f</b> of document
      <b>d</b> is calculated as follows: 
      <div class="IndentL"> 
        <table cellpadding="0" cellspacing="0" border="0">
          <tr> 
            <td valign="middle" align="right" rowspan="2">metaScore(q,d,f) = lengthNorm(d,f) *
              tf(<br/>
            </td>
            <td valign="middle" align="center"> <big><big><big><big>&Sigma;</big></big></big></big></td>
            <td valign="middle"><small> spanScore(q,s)</small></td>
            <td valign="middle" align="right" rowspan="2"> &nbsp;) <br/></td>
          </tr>
          <tr> 
            <td valign="top" align="right"> <small>s in d.f</small> </td>
          </tr>
        </table>
      </div>
      where 
      <ul>
        <li> <b>lengthNorm(d,f)</b> is a factor that decreases as the 
          length of the field increases, since matches 
          in longer fields are generally less precise. Default implementation: 
          square root of the number of terms in the field. </li>
        <li> <b>tf(...)</b> is a score factor that helps to equalize the scoring
          between fields with many matches and those with few matches.
          Default implementation: 
          square root of the total score of the matches in <b>d.f</b>. </li>
      </ul>
    </div>
    <p class="Heading2">Combined Document Score<a name="Scoring_Documents"></a></p>
    <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
    The final type of scoring <b><i>XTF</i></b> performs is to combine the scores 
    of all text hits with a document with that document's meta-data scores, to 
    form the final score for that document. Again, the structure of this computation 
    is fixed, but the calculations can be overridden by providing a Java implementation 
    of the <b> Similarity</b> interface. 
    <div class="IndentL"> 
      <p class="Heading3">Plain English</p>
      <ul>
        <li> A document's score is based on the sum of all the text hits within 
          it, plus its meta-data field scores. </li>
      </ul>
      <p class="Heading3">Mathematical Details</p>
      <br/>
      For a given query <b>q</b> consisting of meta-data queries <b>qm<sub>f</sub></b> on fields <b>f</b>, and 
      a text query <b>qt</b>, the score for a specific document <b>d</b> is as follows: 
      <div class="IndentL"> 
        <table cellpadding="0" cellspacing="0" border="0">
          <tr> 
            <td valign="middle" align="right" rowspan="2"> docScore(q,d) = textScore(qt,d) +
              <br/>
            </td>
            <td valign="middle" align="center"> <big><big><big><big>&Sigma;</big></big></big></big></td>
            <td valign="middle"><small> metaScore(qm<sub>f</sub>,d,f) </small></td>
          </tr>
          <tr> 
            <td valign="top" align="right"> <small>f in d</small> </td>
          </tr>
        </table>
      </div>
      <br/>
      where <b>metaScore</b> and <b>textScore</b> are computed as outlined in 
      the previous two sections. </div>
  </div>
  <!-- IndentL --> 
  <p class="Heading1">Spelling Correction<a name="Spelling"></a></p>
  <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
  <p> 
    Everybody likes Google's "did you mean" suggestions. Users often misspell words when
    they're querying an XTF index, and it would be nice if the system could catch the most
    obvious errors and automatically suggest an appropriate spelling correction. The XTF team
    did extensive work to create a fast and accurate facility for doing this, involving minimal
    work for those deploying XTF. 
  </p>
  <p>
    In the following sections, we'll discuss the guts of XTF's spelling correction
    system, detailing some strategies that were
    considered and the final strategy selected, the methods that XTF uses
    to come up with high-quality suggestions, and how the
    dictionary is created and stored. If you're looking for information
    on configuring and using the system, see the 
    <a href="programming.html">Programming Guide</a>.
  </p>
  <div class="IndentL"> 
    <p class="Heading2">Choosing an Index-based Strategy<a name="Spelling_Strategy"></a></p>
    <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
    <p> 
      We considered three strategies for spelling correction, each deriving suggestions from
      a different kind of source data.
    </p>
    <ol>
      <li>
        <i>Fixed Dictionary</i>: Available software, such as GNU Aspell, makes spelling corrections 
        based on one or more fixed, pre-constructed dictionaries. But the
        bibliographic data in our test bed and our sample queries from a live university
        library catalog were multilingual and contained a substantial proportion of proper nouns (e.g.
        names of people or places). This ruled out a fixed dictionary, since
        many of these proper nouns and foreign words wouldn't be present in any standard
        dictionary.<br/><br/>
      </li>
      <li>
        <i>Query Logs</i>: In this method, we would compare the user's query to an extensive
        set of prior queries, and suggest alternative queries which resulted in more hits.
        Unfortunately, our test system has a limited number of users, and there was
        no feasible way to get hold of the millions of representative queries this strategy
        would require, so it was ruled out as well.<br/><br/>
      </li>
      <li>
        <i>Index-based Dictionary</i>: Here we would make
        suggestions using an algorithm that draws on terms and term frequency data from the XTF
        index itself. It resembles the first approach except that the dictionary
        is dynamically derived from the actual documents and their text.
      </li>
    </ol>
    <p>
      Because of the issues identified with the other strategies, we opted to pursue the index-based approach.
      We feel it is best for most collections in most
      situations, as it adapts to the documents most germane to the application and users,
      and doesn't require a long query history
      to become effective.
    </p>
    <p> 
      We set ourselves a goal of getting the correct suggestion in the number 1 position 80% of the time,
      which seemed a good threshold for the system to be truly useful.
      With several iterations and many tweaks to the algorithm, we were able to achieve this goal for
      our sample data set and our sample queries (drawn from real query logs). We have a fair degree
      of confidence that the algorithm is quite general and should be applicable to many other
      data sets and types of queries. 
    </p>
  </div>
  <div class="IndentLR"> 
    <p class="Heading2">Correction Algorithm<a name="Spelling_CorrectionAlg"></a></p>
    <p>
      <b><i>XTF</i></b> builds a spelling correction dictionary at index-time. If a query is
      sent to <b><i>crossQuery</i></b> and results in only a small number of hits (the
      threshold is configurable), XTF consults the dictionary to make an alternative
      spelling suggestion. Here is a brief outline of the algorithm for making a suggestion:
    </p>
    <ol>
      <li>For each word in the query, make a list of candidate suggestions.</li>
      <li>Rank the suggestions and pick the best one (details below).</li>
      <li>For multi-word queries, consider pair frequencies to improve quality.</li>
      <li>Suppress near-identical suggestions, and ensure that the final suggestion 
        increases the number of hits.</li>
    </ol>
    Each of these will be considered in more detail below.
    
    <div class="IndentL"> 
      <p class="Heading3">Which Words to Consider?<a name="Spelling_Candidates"></a></p>
      <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
      <p>
        Most spelling algorithms, including this one, rely on a sort of "shot-gun"
        approach: for each potentially mispelled word in the query, they make a long
        list of "candidate" words, that is, words that could be suggested as a
        correction for the original misspelled word. 
        Then the list of candidates is ranked using some sort of
        scoring formula, and finally the top-ranked candidate is presented to the user.
      </p>
      <p>
        One might naively attempt to scan <i>every</i> word in the dictionary as a
        candidate. Unfortunately, the cost of scoring each one becomes prohibitive
        when the dictionary grows beyond about ten thousand words. So a strategy is needed
        to quickly come up with a list of a few hundred pretty good candidates. XTF
        uses a novel approach that gives good speed and accuracy.
      </p>
      <p>
        We began with a base of existing Java spelling correction code that had been contributed 
        to the Lucene project (written by Nicolas Maisonneuve, based on code originally 
        contributed by David Spencer).  The base Lucene algorithm first breaks up the word 
        we're looking for into 2, 3, or 4-character "n-grams" (for instance, the word <span class="Code">primer</span> might end up as: 
        <span class="Code">~pri prim rime imer mer~</span> ). Next, it performs a Lucene OR query on
        the dictionary (also built with n-grams), retaining the top 100 hits (where a hit 
        represents a correctly spelled word that shares some n-grams with the target word). 
        Finally, it ranks the suggestions according to their "edit distance" to the original 
        misspelled word. Those that are closest appear at the top. ("Edit distance" is a
        standard numerical measure of how far two words are from each other and is
        defined as the number of insert, replace, and delete operations needed to transform 
        one word into the other.)
      </p>
      <p>
        Unfortunately, the base method was quite slow, and often didn't find the
        correct word, especially in the case of short words. However, we made one
        critical observation: In
        perhaps 85-90% of the cases we examined, the correct word had an edit distance of 1 or 2
        from the misspelled query word; another 5-10% had an edit distance of 3, and the extra edit 
        was usually toward the end of the word. This observation intuitively rings true:
        the suggested word should be relatively "close" to the
        query word and those that are "far" away needn't even be considered.
      </p>
      <a name="Spelling_EditKeys"/>
      <p>
        Still, one wouldn't want to consider <i>all possible</i> one- and two-letter
        edits as it would still take a long time to check if all those possible
        edits were actually in the dictionary. Instead, XTF checks all
        words in which four of the first six characters match in order. This effectively checks
        for an edit distance of two or less at the start of the word.
      </p>
      <p>
        Take for example the word <span class="Code">GLOBALISM</span>. Here are the 15 keys that
        can be created by deleting two of the first six characters:
      </p>
      <div class="Sample">
        <ol>
          <li><span class="Code">OBAL</span> <span class="MacroCode">(deleted characters 1 and 2)</span></li>
          <li><span class="Code">LBAL</span> <span class="MacroCode">(deleted characters 1 and 3)</span></li>
          <li><span class="Code">LOAL</span> <span class="MacroCode">(deleted characters 1 and 4)</span></li>
          <li><span class="Code">LOBL</span> <span class="MacroCode">(deleted characters 1 and 5)</span></li>
          <li><span class="Code">LOBA</span> <span class="MacroCode">(deleted characters 1 and 6)</span></li>
          <li><span class="Code">GBAL</span> <span class="MacroCode">(deleted characters 2 and 3)</span></li>
          <li><span class="Code">GOAL</span> <span class="MacroCode">(deleted characters 2 and 4)</span></li>
          <li><span class="Code">GOBL</span> <span class="MacroCode">(deleted characters 2 and 5)</span></li>
          <li><span class="Code">GOBA</span> <span class="MacroCode">(deleted characters 2 and 6)</span></li>
          <li><span class="Code">GLAL</span> <span class="MacroCode">(deleted characters 3 and 4)</span></li>
          <li><span class="Code">GLBL</span> <span class="MacroCode">(deleted characters 3 and 5)</span></li>
          <li><span class="Code">GLBA</span> <span class="MacroCode">(deleted characters 3 and 6)</span></li>
          <li><span class="Code">GLOL</span> <span class="MacroCode">(deleted characters 4 and 5)</span></li>
          <li><span class="Code">GLOA</span> <span class="MacroCode">(deleted characters 4 and 6)</span></li>
          <li><span class="Code">GLOB</span> <span class="MacroCode">(deleted characters 5 and 6)</span></li>
        </ol>
      </div>
      <p>
        So, XTF checks each of the 15 possible 4-letter keys for a given
        query word, and makes a merged list of all the words that share those same keys.
        This combined list is usually only a few hundred words long, and almost always contains
        within it the golden "correct" word we're looking for.
      </p>
    </div>
    
    <div class="IndentL"> 
      <p class="Heading3">Ranking the Candidates<a name="Spelling_Scoring"></a></p>
      <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
      <p>
        Given a list of candidate words, how does one find the "right" suggestion?
        This is the heart of most spelling algorithms and the area that needs
        the most "tweaking" to achieve good results. XTF's ranking algorithm is
        no exception, and makes decisions by assigning a score to each candidate word. 
        The score is a sum of the following factors:
      </p>
      <ol>
        <li>
          <b>Edit distance.</b> The base score is calculated from the edit distance 
          between the query word and the candidate word. The usual edit distance 
          algorithm (which considers only insertion, deletion, and replacement) is 
          supplemented by reducing the
          "cost" of transposition and double-letter errors. Those types
          of errors are very common spelling mistakes, and reducing their
          cost was shown by our testing to improve suggestion accuracy. In math terms,
          the base score is calculated as 1.0 - (editDistance / queryWord.length).
          <br/><br/>
        </li>
        <li>
          <b>Metaphone.</b> 
          Originally developed by Lawrence Philips, the Double Metaphone algorithm is a simple 
          phonetic mapping that transforms any English word into a 4 character code (metaphone). 
          Words that have the same code are assumed to be phonetically similar. In XTF's
          spelling system, if the metaphone of the candidate word matches that of the query word,
          the score is nudged upward by 0.1. (Note: Philips'
          algorithm actually produces two codes, primary and secondary, for a given
          input word; XTF only compares the primary code.)
          <br/><br/>
        </li>
        <li>
          <b>First/last letter.</b> If the first and last letters of the candidate word 
          match those of the query word, then the score is nudged upward by 0.1. Again, testing 
          showed that this approach boosted overall accuracy.
          <br/><br/>
        </li>
        <li>
          <b>Frequency.</b> Because the indexes do contain misspelled words, we further boost the score 
          if the suggested word is very common. The boost ranges from 0.01 to 0.2, and
          is arrived at by a slightly complex method, but basically more frequent candidate 
          words get higher boosts. This has the effect of favoring common words over
          uncommon words (other factors being equal), which helps to avoid ridiculous suggestions.
        </li>
      </ol>
      <p>
        The original query word itself is always considered as one of the candidates.
        This is to reduce the problem of suggesting replacements for correctly spelled
        words. However, the score of the query word is reduced slightly in case a more
        common word is found that is very similar.
      </p>
      <p>
        In summary, for a single-word query, the list of all candidates is ranked by score, and the one 
        with the highest score wins and is considered the "best" suggestion for the user.
      </p>
    </div>
    
    <div class="IndentL"> 
      <p class="Heading3">Multi-word Correction<a name="Spelling_MultiWord"></a></p>
      <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
      <p>
        But what about queries with multiple words? Testing showed that considering each
        word of a phrase independently and concatenating the result got plenty of
        important cases wrong. For instance, consider the query <span class="Code">untied states</span>. Actually,
        each of these words is spelled correctly, but it's clear the user probably meant
        to query for <span class="Code">united states</span>. Also, consider the word <span class="Code">harrypotter</span>... the best
        single-word replacement might be <span class="Code">hairy</span>, but that's not what the user meant.
        How do we go beyond single-word suggestions?
      </p>
      <p>
        We need to know more than just the frequency of the words in the index; we need
        to know how often they occur <i>together</i>. So when XTF builds the spelling
        dictionary, it additionally tracks the frequency of each pair of words as they occur
        in sequence.
      </p>
      <p>
        Using the pair frequency data, we can take a more sophisticated approach to
        multi-word correction. Specifically, XTF tries the following additional
        strategies to see if they yield better suggestions:
      </p>
      <ol>
        <li>
          For each single word (e.g. <span class="Code">harrypotter</span>) we consider splitting it up into
          pairs and see if any of those pairs has a high frequency. For example
          we'd check &lt;<span class="Code">har rypotter</span>&gt;, 
          &lt;<span class="Code">harr ypotter</span>&gt;, 
          &lt;<span class="Code">harry potter</span>&gt;, etc. and get
          a solid hit on that last one. That "hitting" takes the form of a higher
          score boost based on the pair frequency.
          <br/><br/>
        </li>
        <li>
          For each consecutive pair of words, we fetch the top 100 single-word suggestions for
          both words in the pair. Then we consider all possible combinations of a word from
          the first list vs. a word from the second list
          (that's 10,000 combinations) to see if any of the combinations are high-frequency
          pairs. So in the example of &lt;<span class="Code">untied states</span>&gt;, one of the suggestions for
          <span class="Code">untied</span> will certainly be <span class="Code">united</span> 
          and we'll discover that &lt;<span class="Code">united states</span>&gt;
          as a pair has a high frequency. So we boost the score for <span class="Code">united</span>, and
          it ends up winning out against the other candidates.
          <br/><br/>
        </li>  
        <li>
          Finally, we consider joining each pair of words together. So if a user
          accidentally queries for &lt;<span class="Code">uni lateralism</span>&gt;, we have a good chance of
          suggesting <span class="Code">unilateralism</span>.
        </li>
      </ol>
      All of these strategies are attempted and, just as in single-word candidate
      selection, each phrase candidate is scored and ranked. The highest 
      scoring phrase wins.
    </div>
    
    <div class="IndentL"> 
      <p class="Heading3">Sanity Checks<a name="Spelling_Sanity"></a></p>
      <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
      <p>
        Despite all the above efforts, sometimes the spelling system makes bad
        suggestions. A couple of methods are used to minimize these.
      </p>
      <p>
        First, a filter is applied to avoid suggesting "equivalent" words. In XTF, the indexer
        performs several mapping functions, such as converting plural words to singular
        words, and mapping accented characters to their unaccented equivalents. It would
        be silly for the spelling system to suggest <span class="Code">cat</span> if the user entered <span class="Code">cats</span>,
        even if <span class="Code">cat</span> would normally be considered a better suggestion because it has
        higher frequency. The final suggestion is checked, and if it's equivalent (i.e.
        maps to the same words) to the user's initial query, no suggestion is made.
      </p>
      <p>
        Second, it's quite possible for the spelling correction system to make a 
        suggestion that
        will yield fewer results than the user's original query. While this isn't
        common, it happens often enough that it could be annoying. So after the
        spelling system comes up with a suggestion, XTF automatically runs the 
        resulting modified query, and if fewer results are obtained, the suggestion is
        simply suppressed.
      </p>
    </div>
  </div>
  <div class="IndentL"> 
    <p class="Heading2">Dictionary Creation<a name="Spelling_DictCreation"></a></p>
    <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
    <p>
      Now we turn to the dictionary used by the algorithm above to produce
      suggestions. How is it created during the XTF indexing process?
      Let's find out.
    </p>
    <div class="IndentL"> 
      <p class="Heading3">Incremental Approach<a name="Spelling_Incremental"></a></p>
      <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
      <p>
        One of the best features of the XTF indexer is incremental indexing, the
        ability to quickly add a few documents to an existing index. So we needed
        an incremental spelling dictionary build process to stay true to the
        indexer's design. To do this we phase the work, with a low-overhead
        collection pass added to the main indexing process, and then a phase of
        intensive dictionary generation, optimized as much as possible.
      </p>
      <p>
        During the main index run, XTF simply
        collects information on which words are present, their frequencies,
        and the frequency of pairs. Data is collected in a fairly small
        RAM cache and periodically sorted and written to disk.
        Two filters assure that we avoid accumulating
        counts for rare words and rare word pairs. Words that occur only
        once or twice are disregarded (though this limit is configurable);
        likewise, pairs that occur only once or twice are not written
        to disk. Collection adds minimal CPU overhead to indexing.
      </p>
      <p>
        Then comes the dictionary creation phase, which processes
        the queued word and pair counts to form the final
        dictionary (this can optionally be delayed until another index run).
        Here are the processing steps:
      </p>
      <ol>
        <li>
          Read in the list of words and their frequencies, merging any existing
          list with new words added since the last dictionary build.
          <br/><br/>
        </li>
        <li>
          Sort the word list and sum up frequency counts for identical words.
          <br/><br/>
        </li>
        <li>
          Sample the frequency distribution of the words.
          This enables the correction algorithm to quickly
          decide how much boost a given word should receive by ranking its
          frequency in relation to the rest of the words in the dictionary.
          <br/><br/>
        </li>
        <li>
          Create the "edit map" file. For each word, calculate each of the
          15 edit keys, and add the word to the list for that key. 
          Key calculation is discussed 
          <a href="#Spelling_EditKeys">above</a>.
          This file is created and then sorted on disk and the lists 
          for duplicate entries are merged together.
          <br/><br/>
        </li>
        <li>
          Finally, any existing pair data from a previous dictionary
          creation run is read in and then new pairs are added from the
          most recent main index phase. Unlike the other parts of
          dictionary creation, this work is all done in RAM, for reasons
          outlined in the next section covering data structures.
        </li>
      </ol>
    </div>
    <div class="IndentL"> 
      <p class="Heading3">Data Structures<a name="Spelling_DataStruct"></a></p>
      <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
      <p>
        The data structures used to store the dictionary are motivated by the
        needs of the spelling correction algorithm. In particular, it needs
        the following information to make good suggestions:
      </p>
      <ol>
        <li>
          The list of words that are "similar" to a given word (i.e. whose
          beginning is an edit distance of 2 or less.)
        </li>
        <li>
          The frequency of a given candidate word (i.e. how often that word
          occurs within the documents in an index.)
        </li>
        <li>
          The frequency of a given pair of words (i.e. how often the two
          words appear together, in sequence, in the index.)
        </li>
      </ol>
      To support these needs, the dictionary consists of three main
      data structures, each of which is discussed below.
      <ul>
        <li>
          <p>
            <b>Edit Map.</b> Since at most
            15 keys need to be read for a given input word, this data structure
            is mainly disk-based (it's never read entirely into RAM.) The disk
            file consists of one line per 4-letter key, listing all the words
            that share that key. At the end of the file is an index giving
            the length (in bytes) for each key, so that the correction
            engine can quickly and randomly access the entries.
          </p>
          <p>
            The words in each list are <i>prefix-compressed</i> to conserve
            disk space. This is a way of compressing a list of words when
            many of them share prefixes. For example, say we have three
            words in the list for a key:
          </p>
          <div class="Sample">
            <span class="CodeNorm">apple application aplomb</span>
          </div>
          <p>
            We always store
            the first word in its entirety; each word after that is stored
            as the number of characters it has in common with the previous word,
            plus the characters it doesn't share. For long lists of similar words, the
            compression becomes quite significant. In our example the compressed list is:
          </p>
          <div class="Sample">
            <span class="CodeNorm">apple <b>4</b>ication <b>2</b>lomb</span>
          </div>   
          <p>
            Here are some lines from a real edit map file, with the keys in <b>bold</b>:
          </p>
          <div class="Sample">
            <span class="CodeNorm">
              <b>abrd</b>|aboard|2road|2surd|6ist|7ty|6ly<br/>
              <b>abre</b>|abbreviated|9ions|0fabre|0laborer|7s<br/>
              <b>abrg</b>|aboriginal<br/>
              <b>abrh</b>|abraham|7son<br/>
              <b>abri</b>|aboriginal|4tion|8s|0cambridge|0fabric|6ated|0labyrinth<br/>
              <span class="MacroCode">... and the random-access index at the end of the file ...</span><br/>
              <b>abrd</b>|37<br/>
              <b>abre</b>|42<br/>
              <b>abrg</b>|16<br/>
              <b>abrh</b>|18<br/>
              <b>abri</b>|61<br/>
            </span>
          </div>
        </li>
        <li>
          <p>
            <b>Word Frequency Table.</b> On disk
            this is stored as a simple text file with one line per word giving its 
            frequency. The lines are sorted in ascending word order. This structure
            is read completely into RAM by the correction engine, as we need to
            potentially evaluate tens of thousands of candidate words per second in a
            high-volume application.
          </p>
          <p>
            Here are some lines from a real word frequency file:
          </p>
          <div class="Sample">
            <span class="CodeNorm">
              aboard|10<br/>
              abolished|19<br/>
              abolishing|9<br/>
              abolish|8<br/>
              abolition|34<br/>
              abomination|6<br/>
            </span>
          </div>
        </li>
        <li>
          <p>
            <b>Pair Frequency Table.</b> The correction engine needs to check the frequency of
            hundreds of thousands of word pairs per second. This implies a need for
            extremely fast access, so we need to pull the entire data structure
            into RAM and search it very quickly.
          </p>
          <p>
            The table exists only in binary (rather than text) form, in a very simple
            structure. A "hash code" is computed for each pair of words. The hash code is
            a 64-bit integer that does a good job of characterizing the pair; for
            two different pairs, the chance of getting the same hash code is
            vanishingly small. The structure consists of a large array of the
            all the pairs' hash codes, sorted numerically, plus a frequency count per pair. 
            This sorted data is amenable to a fast in-memory binary search.
          </p>
          <p>
            Here's the disk layout:
          </p>
          <table border="1" cellpadding="5" class="Sample">
            <tr><td><b># bytes</b></td><td><b>Description</b></td></tr>
            <tr><td>8</td><td>Magic number (identifies this as a pair frequency file)</td></tr>
            <tr><td>4</td><td>Total number of pairs in the file</td></tr>
            <tr><td>8</td><td>Hash code of pair 1</td></tr>
            <tr><td>4</td><td>... and frequency of pair 1</td></tr>
            <tr><td>8</td><td>Hash code of pair 2</td></tr>
            <tr><td>4</td><td>... and frequency of pair 2</td></tr>
            <tr><td>8</td><td>Hash code of pair 3</td></tr>
            <tr><td>...</td><td>etc.</td></tr>
          </table>
          <p>
            As you can see, we store each pair with exactly 12 bytes: 8 bytes
            for the 64-bit hash code, and 4 bytes for the 32-bit count. Working with
            fixed-size chunks makes the code simple, and also keeps
            the pair data file (and corresponding RAM footprint) relatively small.
          </p>
        </li>
      </ul>
      
    </div>
  </div>
  <!-- BaseStyle --> 
  <p class="Heading1">Lazy Trees<a name="LazyFiles"></a></p>
  <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
  To accelerate processing of large documents and to enable highlighting search 
  results within a document, <b><i>XTF</i></b> creates and utilizes <i>lazy trees</i>. 
  This section describes what lazy trees are, how they aid processing, and how 
  stylesheets should optimize for their use. 
  <div class="IndentL"> 
    <p class="Heading2">The Problem of Large Documents<a name="Large_Documents"></a></p>
    <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
    <p>The user browses a document page by page – for instance, reading a section, 
      then jumping to an appendix, then another section, etc. Each time the servlet 
      produces one of these “pages”, it must read and process the entire source 
      document, even though only a small portion contributes to the output. This 
      is the key observation underlying lazy trees.</p>
    <p>When a document is processed through a set of stylesheets, the XSLT processor 
      spends significant time reading and parsing the XML document, building indexes 
      of parent-child relationships, creating identifier cross-references, and 
      so on. Most of this work is wasted in generating the page for a single section, 
      so what if we could eliminate the data for all the other sections? </p>
    <p>Let’s take an example. Consider the source document below (simplified for 
      discussion). If the user wants to see Chapter 1, only the elements shown 
      with a darker background are actually needed. </p>
    <pre class="Sample">&lt;front&gt;
  &lt;titlePage&gt;The opening of the Apartheid Mind&lt;/titlePage&gt;
  blah… blah… blah…
&lt;/front&gt;
  &lt;div1 id="ack"&gt;
  &lt;head&gt;Acknowledgements&lt;/head&gt;
  &lt;p&gt;I would like to thank…&lt;/p&gt;
  blah, blah, blah
  blah, blah, blah
&lt;/div1&gt;
<span class="DarkSample"><span class="RedB">&lt;div1 id="ch1"&gt;
  &lt;head&gt;Chapter 1&lt;/head&gt;
  &lt;p&gt;It is now conventional wisdom that…&lt;/p&gt;
  &lt;p&gt;See Hanlon (1981) for an overview… &lt;ref target="bib12"/&gt;&lt;/p&gt;
  blah, blah, blah
  blah, blah, blah
&lt;/div1&gt;</span></span>
&lt;div1 id="ch2"&gt;
  &lt;head&gt;Chapter 2&lt;/head&gt;
  &lt;p&gt;One of the more striking aspects of contemporary…&lt;/p&gt;
  blah, blah, blah
  blah, blah, blah
&lt;/div1&gt;
<span class="DarkSample">&lt;div1 id="bib"&gt;</span>  
  &lt;bibl id="bib1"&gt;  &lt;author&gt;Adams, Heribert&lt;/author&gt; &lt;/bibl&gt;
  blah, blah, blah
  blah, blah, blah
  <span class="DarkSample">&lt;bibl id="bib12"&gt; &lt;author&gt;Hanlon, Joseph&lt;/author&gt; &lt;/bibl&gt;</span>
  blah, blah, blah
  blah, blah, blah
<span class="DarkSample">&lt;/div1&gt;</span></pre>
    <p>Now that might not look like much of a savings, until you remember that 
      each "blah, blah, blah" stands for a great deal of data.</p>
    <p>The red text shows the actual data for Chapter 1. But all the other dark-shaded 
      lines must also be included. Why? Notice that Chapter 1 contains a bibliographic 
      reference; in order to properly generate a hyperlink to it, that reference 
      element ("bib12") must be included. </p>
    <p>In general, the only parts of the document needed are those the stylesheet 
      actually references while generating a given page. This leads to the central 
      idea of lazy trees: only load those parts of the document that are needed.</p>
    <p class="Heading2">What is a &quot;Lazy Tree&quot;?<a name="What_is_lazy"></a></p>
    <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
    <p></p>
    <p>Unfortunately it's not easy to randomly load only pieces of an XML document. 
      In general, one can't tell where the end tag for an element is without scanning 
      all the text until it's found. So <i><b>XTF </b></i>creates a binary version 
      of each XML document, called a &quot;lazy tree&quot;. The tree is stored 
      in a file containing all the original contents of the document, plus an 
      index telling <i><b>XTF </b></i>where each element starts and ends.</p>
    <p>Processing begins by loading only the root element of the document. If 
      the stylesheet references the children of that element (in an XPath expression 
      for instance), then <b><i>XTF</i></b> knows right where to find them in 
      the lazy tree's file. It loads only those children, but not their descendants. 
      Processing then proceeds again until the stylesheet needs another node that 
      hasn't been loaded yet. In this way, a typical page generation ends up loading 
      only a small portion of the document.</p>
    <p>This strategy makes it possible to quickly generate pages for any size 
      document (tested up to at least 60 gigabytes).</p>
    <p>Lazy tree files are stored under the index directory, in a folder hierarchy 
      parallel to that of the original data directories. They are typically a 
      bit smaller than the source document, since the text within them is compressed.</p>
    <p class="Heading2">Search Results in Context<a name="Search_results_in_context"></a></p>
    <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
    <p>The other main benefit of lazy trees is that they enable <b><i>XTF</i></b> 
      to show search results in context. At first the link might seem unobvious. 
      Remember that <b><i>XTF</i></b> reports, for each element, the total number 
      of hits found in that element <i>and all of its descendants</i>. Without 
      a lazy tree, the system would have to read in the entire XML document to 
      determine the parent-child relationships. With a lazy tree, each search 
      hit can be directly attributed to the correct XML elements by simply looking 
      them up in the cross-index stored in the lazy tree's file.</p>
    <p>For this reason and for speed of processing, <b><i>dynaXML</i></b> always 
      uses the lazy tree if one has been created. If it doesn't exist yet, <b><i>dynaXML</i></b> 
      will attempt to create it at run-time. This introduces a pause, which can 
      be avoided by having the <b><i>textIndexer</i></b> can create lazy trees 
      at index time (and in fact, this is the default behavior.) One caution: 
      if pre-building of lazy trees by the <b><i>textIndexer</i></b> is disabled, 
      be sure that <b>docSelector.xsl</b> and <b>docReqParser.xsl</b> both specify 
      the same pre-filter to use. Otherwise, <b><i>dynaXML</i></b> will build a 
      lazy tree that doesn't match the index, and strange random errors will occur 
      when highlighting search hits in the document.</p>
    <p></p>
    <p class="Heading2">Stylesheet Considerations<a name="Stylesheet_considerations"></a></p>
    <!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
    <p></p>
    <p>Though the process of loading elements and other pieces of an XML file 
      is generally invisible to the stylesheet programmer, there are certain best 
      practices to obtain maximum processing speed.</p>
    <p>First, avoid using XSL constructs that scan the entire input document. 
      Generally, any XPath instruction beginning with &quot;//&quot; will scan 
      every element of the document, defeating any gains of using a lazy tree. 
      Also, using &quot;descendant::&quot; and &quot;descendant-or-self::&quot; 
      in an XPath expression will generally cause all of the descendants to be 
      loaded, again counteracting the benefit of lazy trees.</p>
    <p>Instead, try to replace these constructs with the use of XSL keys, declared 
      with the &lt;xsl:key&gt; element at the top-level of the stylesheet. You 
      might ask, &quot;Doesn't xsl:key need to scan every element of the document 
      to build the key?&quot; The answer is yes, but the result is stored in the 
      lazy document, so that subsequent page views using the same key don't need 
      to scan the document again. In other words, only the first page view causes 
      XSL keys to be built.</p>
    <p>Even the overhead of building XSL keys at runtime can be avoided by having 
      the <b><i>textIndexer</i></b> do that work. This is accomplished simply 
      by specifying the <i>displayStyle </i>attribute in the <b>docSelector.xsl</b> 
      stylesheet used by the indexer. If specified, the stylesheet referenced 
      by <i>displayStyle</i> will be scanned for &lt;xsl:key&gt; declarations, 
      and all of the keys will be pre-built at index time. See the <a href="programming.html#textIndexer_DocSelector_Prog">Document 
      Selector Programming</a> section in the <b>XTF Programming Guide</b>.</p>
    <p>One final note: a good way to locate parts of the stylesheet that need 
      to be optimized is to use <b><i>dynaXML</i></b>'s stylesheet profiling configuration 
      option. When enabled, a summary of how many XML document nodes were accessed 
      by each line of the stylesheet will be printed. Find a particular request 
      that runs unacceptably long, examine the profile, and take aim at the lines 
      which access the most nodes.</p>
    <p>&nbsp;</p>
  </div>
</div>
</body>

</html>
